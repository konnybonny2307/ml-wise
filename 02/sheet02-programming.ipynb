{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table width='100%'>\n",
    "<tr>\n",
    "<td style='background-color:white'>\n",
    "    <p align=\"left\">\n",
    "    Exercises for the course<br>\n",
    "        <b>Machine Learning for Data Science</b><br>\n",
    "    Winter Semester 2024/25\n",
    "    </p>\n",
    "</td>\n",
    "<td style='background-color:white'>\n",
    "    G. Montavon<br>\n",
    "    Institute of Computer Science<br>\n",
    "    <b>Department of Mathematics and Computer Science</b><br>\n",
    "    Freie Universit√§t Berlin\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <h1>Exercise Sheet 2 (programming part)</h1>\n",
    "</center>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load a small handwritten digits dataset provided as part of the scikit-learn library and a few relevant libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy\n",
    "import torch,torch.optim\n",
    "import scipy,scipy.spatial\n",
    "import sklearn, sklearn.datasets\n",
    "\n",
    "dataset = sklearn.datasets.load_digits()\n",
    "X = dataset['data']\n",
    "T = dataset['target']\n",
    "R = numpy.random.mtrand.RandomState(0).permutation(len(X))[:750]\n",
    "X = X[R]\n",
    "T = T[R]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Implementing Metric MDS (15+15+10 P)\n",
    "\n",
    "In this exercise, we will implement a low-dimensional embedding technique, multi-dimensional scalling (MDS), apply it to the simple handwritten digits dataset above, and visualize the embedded data using a series of scatter plots.\n",
    "\n",
    "The MDS algorithm does not operate with the dataset directly, but instead, with a matrix of distances. Also, in order to be rather independent from the scale and dimensionality of the data, we would like to normalize the input distance such that distances are on average 10. Lastly, the distance should be provided as a torch tensor so that it can be used for the gradient-based training procedure. \n",
    "\n",
    "**(a)** Write a function that takes the data as input and returns a distance matrix according to the specifications above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: torch.Size([750, 750])\n",
      "min:    0.000\n",
      "mean:  10.000\n",
      "max:   15.391\n"
     ]
    }
   ],
   "source": [
    "D = scipy.spatial.distance.cdist(X,X)\n",
    "\n",
    "t = torch.tensor(D, dtype=torch.float32)\n",
    "\n",
    "# normalize distances to mean 10\n",
    "\n",
    "avg = torch.mean(t)\n",
    "normalization = 10 / avg.item()\n",
    "normD = t * normalization\n",
    "\n",
    "\n",
    "print(f\"shape: {str(normD.shape)}\")\n",
    "print(f\"min:   {normD.min():6.3f}\")\n",
    "print(f\"mean:  {normD.mean():6.3f}\")\n",
    "print(f\"max:   {normD.max():6.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now would like learn Metric MDS. We will consider a simple gradient-based implementation. The outline of the gradient-based procedure is provided in the code below. The only part missing is the computation of the objective $J(Y)$ where $Y$ are the matrix of size $N \\times 2$ containing the coordinates of data points in embedded space. As a reminder, the metric MDS objective the is given by:\n",
    "$$J(\\boldsymbol{y}_1,\\dots,\\boldsymbol{y}_N) = \\left(\\frac{\\sum_{i<j} \\big(d_{ij} - \\|\\boldsymbol{y}_i - \\boldsymbol{y}_j\\|\\big)^2}{\\sum_{i<j} d_{ij}^2}\\right)^{\\frac12}$$\n",
    "\n",
    "**(b)** Complete the code below by expressing `J` as a function of `Y` and `D`.\n",
    "\n",
    "*(Hint: Running the training procedure may take up to 5 minutes. For debugging purposes, you can temporarily reduce the number of iterations.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 0.487\n",
      "    1 0.486\n",
      "   16 0.459\n",
      "   81 0.424\n",
      "  256 0.412\n",
      "  625 0.375\n",
      " 1296 0.352\n",
      " 2401 0.339\n",
      " 4096 0.337\n",
      " 6561 0.337\n",
      "10000 0.337\n"
     ]
    }
   ],
   "source": [
    "Y = torch.randn([len(X),2])*5\n",
    "Y.requires_grad_(True)\n",
    "\n",
    "optimizer = torch.optim.SGD((Y,), lr=100.0, momentum=0.95)\n",
    "\n",
    "Yhist = []\n",
    "\n",
    "for i in range(10**4+1):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    low_dist = torch.cdist(Y, Y, p=2)\n",
    "    \n",
    "    # compute the difference between the distances\n",
    "    diff_dist = normD - low_dist\n",
    "    numerator = torch.sum(diff_dist**2)\n",
    "    denominator = torch.sum(normD**2)\n",
    "    \n",
    "    \n",
    "    # objective function\n",
    "    J = torch.sqrt(numerator / denominator)\n",
    "    J.requires_grad_(True)\n",
    "    \n",
    "    # backpropagate\n",
    "    J.backward()\n",
    "\n",
    "    # Keep track of the objective\n",
    "    if i in numpy.arange(11)**4:\n",
    "        print(f\"{i:5d} {J.data.numpy():.3f}\")\n",
    "        Yhist += [Y.data.numpy()*1.0]\n",
    "        \n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above has stored the embedding at 11 distinct checkpoints (available in `Yhist`). This allows to visualize how the embedding (initialized at random) evolves throughout training, and how it progressively progressively improves the representation of distances in the data.\n",
    "\n",
    "**(c)** Using matplotlib, plot the embedding at checkpoints {0,3,4,5,6,10}, and color-code the embedding according to the digit's class *(Hint: because there are 10 classes, you can use of the colormap `tab10` in matplotlib.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "checkpoints = [0, 3, 4, 5, 6, 10]\n",
    "\n",
    "# commit test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that during training, data points travel in embedded space. In the final configuration, embedded data forms clusters of points of same class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
